# **ğŸ§ª Evaluation Framework**

LLM-as-Judge evaluation framework for multi-agent chatbots.


---


## **ğŸ“‘ Table of Contents**

- [Overview](#-overview)
- [Quick Start](#-quick-start)
- [Documentation](#-documentation)
- [Judges](#-judges)
- [Directory Structure](#-directory-structure)
- [Design Decisions](#-design-decisions)


---


## **ğŸ“‹ Overview**

Evaluates chatbot responses using multiple judges that assess different aspects of the response.

![Overview](../assets/diagrams/evaluation/evaluation_README_1.png)


---


## **ğŸš€ Quick Start**

### **Run All Tests**

```bash
python scripts/run_eval.py customer
python scripts/run_eval.py client
```

### **Run Specific Directory**

```bash
python scripts/run_eval.py customer --path data/eval_datasets/customer/browse_products
```

### **Run Single YAML File**

```bash
python scripts/run_eval.py customer --path data/eval_datasets/customer/browse_products/single_turn/search.yaml
```

### **Run Multiple Paths**

```bash
python scripts/run_eval.py customer \
    --path data/eval_datasets/customer/browse_products \
    --path data/eval_datasets/customer/place_order
```


---


## **ğŸ“– Documentation**

| | | |
|:---:|:---:|:---:|
| [ğŸ—ï¸ **Architecture**](architecture.md)<br/>System design and flow | [âš–ï¸ **Judges**](judges/README.md)<br/>Judge types and scoring | [ğŸ“Š **Datasets**](datasets.md)<br/>Test case format |
| [âš™ï¸ **Configs**](configs.md)<br/>Evaluation configuration | [ğŸ“ˆ **Results**](results.md)<br/>Output format | [ğŸ’¡ **Examples**](examples.md)<br/>Real test examples |
| [ğŸ”„ **Iteration**](iteration.md)<br/>Analyze & improve with AI | | |


---


## **âš–ï¸ Judges**

| Judge | Purpose | Used By |
|-------|---------|---------|
| SQL | SQL query correctness | Both |
| Search | Vector search quality | Customer |
| Visualization | Chart generation quality | Client |
| Response Quality | Response relevance and faithfulness | Both |


---


## **ğŸ“ Directory Structure**

```
evaluation/
â”œâ”€â”€ entities.py              # Data classes (TestCase, JudgeResult, etc.)
â”œâ”€â”€ loader.py                # DatasetLoader
â”œâ”€â”€ judges/
â”‚   â”œâ”€â”€ base.py              # BaseJudge
â”‚   â”œâ”€â”€ selector.py          # JudgeSelector
â”‚   â”œâ”€â”€ sql/main.py          # SQLJudge
â”‚   â”œâ”€â”€ search/main.py       # SearchJudge
â”‚   â”œâ”€â”€ visualization/main.py # VisualizationJudge
â”‚   â””â”€â”€ response_quality/main.py # ResponseQualityJudge
â”œâ”€â”€ repositories/
â”‚   â”œâ”€â”€ base.py              # BaseEvaluationRepository
â”‚   â””â”€â”€ main.py              # EvaluationRepository
â”œâ”€â”€ usecases/
â”‚   â””â”€â”€ main.py              # EvaluationService
â””â”€â”€ dependencies/
    â”œâ”€â”€ customer.py          # Customer eval factory
    â””â”€â”€ client.py            # Client eval factory
```


---


## **ğŸ“ Design Decisions**

| Decision | Link |
|----------|------|
| LLM-as-Judge | [why_llm_as_judge.md](../decisions/why_llm_as_judge.md) |
