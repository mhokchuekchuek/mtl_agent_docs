# **ğŸ—ï¸ Evaluation Architecture**

System design and request flow.


---


## **ğŸ“Š Layers**

<details>
<summary>ğŸ“Š Layers</summary>

![Layers](../assets/diagrams/evaluation/evaluation_architecture_1.png)

</details>

| Layer | Location | Responsibility |
|-------|----------|----------------|
| CLI | `scripts/run_eval.py` | Entry point, select chatbot type |
| Dependencies | `evaluation/dependencies/` | Factory, wire components |
| Usecases | `evaluation/usecases/` | Orchestrate evaluation run |
| Repositories | `evaluation/repositories/` | Load data, invoke API, run judges |
| Judges | `evaluation/judges/` | Evaluate specific aspects |


---


## **ğŸ”„ Evaluation Flow**

<details>
<summary>ğŸ“Š Evaluation Flow</summary>

![Evaluation Flow](../assets/diagrams/evaluation/evaluation_architecture_2.png)

</details>


---


## **ğŸ”€ Single-Turn vs Multi-Turn**

| Type | Description | Thread ID |
|------|-------------|-----------|
| Single-turn | One question, one response | New UUID per test |
| Multi-turn | Multiple turns in sequence | Same UUID for all turns |


### ğŸ”„ **Multi-Turn Flow**

![Multi-Turn Flow](../assets/diagrams/evaluation/evaluation_architecture_3.png)

Each turn is evaluated separately, scores are aggregated.


---


## **âš–ï¸ Judge Selection**

Judges are selected based on `expected` fields in test case:

| Expected Field | Judge Activated |
|----------------|-----------------|
| `sql` | SQLJudge |
| `search_results` | SearchJudge |
| `chart` | VisualizationJudge |
| `response_quality` | ResponseQualityJudge |

> ğŸ“ **Note:** If expected field is missing, judge returns `None` (skipped).


---


## **âŒ Negative Cases**

For negative test cases:
- `sql: "null"` â†’ Pass if no SQL generated
- `search_results: []` â†’ Pass if no results found
- `chart: "null"` â†’ Pass if no chart generated



