# **ğŸ” Search Judge Prompt**

LLM-as-Judge prompt for evaluating vector search quality.


---


## **ğŸ·ï¸ Langfuse Name**

`evaluation_judges_search_judge`


---


## **ğŸ“ Location**

[`prompts/evaluation/judges/search_judge.prompt`](../../../../prompts/evaluation/judges/search_judge.prompt)


---


## **ğŸ”— Used By**

- [SearchJudge](../../../evaluation/judges/search.md)


---


## **ğŸ¤– Model**

`gpt-4o`


---


## **ğŸ“¥ Variables**

| Variable | Type | Description |
|----------|------|-------------|
| `question` | string | User's original question |
| `expected_results` | list | Products that should be found |
| `steps` | list | Agent execution steps |


---


## **ğŸ’¡ Purpose**

Extract and evaluate search results:
1. Find `product_search` or `similar_products` tool calls in steps
2. Score relevance (are results relevant to query?)
3. Score coverage (are expected products found?)


---


## **ğŸ“¤ Output Schema**

```json
{
  "extracted_results": ["product1", "product2"],
  "relevance": {
    "score": 0.0-1.0,
    "reasoning": "..."
  },
  "coverage": {
    "score": 0.0-1.0,
    "reasoning": "..."
  }
}
```
